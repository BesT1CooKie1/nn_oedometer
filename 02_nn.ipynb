{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eed3593-c200-4353-96ac-b4508e0aafea",
   "metadata": {},
   "source": [
    "# Template\n",
    "## Load modules & Check PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4c446a-ed81-46b4-a5e4-90f6a55c1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# data processing\n",
    "import random as r\n",
    "from sys import exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb68479-e6da-4410-ae64-a823e686dc13",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db9fdfa-2bdf-425b-a34f-517b7608404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugger: Aktiviert\n",
    "debug_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec504e-2c5c-452e-bc33-0883ac04dc72",
   "metadata": {},
   "source": [
    "## Preloaded Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464f85c5-4347-47b8-a999-876c3405ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_markdown_table(data: dict, title: str = \"Datenübersicht\", include_index: bool = True, round_digits: int = 4):\n",
    "    \"\"\"\n",
    "    Wandelt ein Dictionary mit Listenwerten in eine Markdown-Tabelle für Jupyter Notebooks um.\n",
    "    \n",
    "    - Schlüssel werden als Header genutzt\n",
    "    - Erste Spalte ist ein Index, falls `include_index=True`\n",
    "    - Einzelwerte werden als separate Tabelle unterhalb dargestellt\n",
    "    - Zahlenwerte werden auf eine einstellbare Anzahl an Nachkommastellen gerundet\n",
    "\n",
    "    :param data: Dictionary mit Key-Value-Paaren\n",
    "    :param title: Überschrift für die Tabelle\n",
    "    :param include_index: Falls True, wird eine Index-Spalte erstellt\n",
    "    :param round_digits: Anzahl der Nachkommastellen, auf die Werte gerundet werden sollen\n",
    "    :return: Markdown-String zur Anzeige in Jupyter\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hilfsfunktion zum Runden von Zahlen\n",
    "    def round_value(val):\n",
    "        if isinstance(val, (int, float)):\n",
    "            return round(val, round_digits)\n",
    "        return val\n",
    "\n",
    "    # Listen und einzelne Werte trennen\n",
    "    list_data = {k: v for k, v in data.items() if isinstance(v, list)}\n",
    "    single_values = {k: v for k, v in data.items() if not isinstance(v, list)}\n",
    "\n",
    "    # Falls es Listen gibt, erstelle eine Tabelle mit Index\n",
    "    if list_data:\n",
    "        max_len = max(len(v) for v in list_data.values())  # Längste Liste bestimmen\n",
    "\n",
    "        # Tabellenkopf\n",
    "        md_table = f\"### {title}\\n\\n\"\n",
    "        md_table += \"| \" + (\"Index | \" if include_index else \"\") + \" | \".join(list_data.keys()) + \" |\\n\"\n",
    "        md_table += \"|-\" + (\"-|\" if include_index else \"\") + \"-|\".join([\"-\" * len(k) for k in list_data.keys()]) + \"-|\\n\"\n",
    "\n",
    "        # Datenzeilen\n",
    "        for i in range(max_len):\n",
    "            row = [str(i)] if include_index else []  # Index hinzufügen (optional)\n",
    "            for key in list_data:\n",
    "                if i < len(list_data[key]):\n",
    "                    row.append(str(round_value(list_data[key][i])))\n",
    "                else:\n",
    "                    row.append(\"\")  # Leere Werte für ungleich lange Listen\n",
    "            md_table += \"| \" + \" | \".join(row) + \" |\\n\"\n",
    "    \n",
    "    else:\n",
    "        md_table = \"\"\n",
    "\n",
    "    # Einzelwerte als extra Tabelle darstellen\n",
    "    if single_values:\n",
    "        md_table += \"\\n\\n#### Einzelwerte\\n\\n\"\n",
    "        md_table += \"| \" + \" | \".join(single_values.keys()) + \" |\\n\"\n",
    "        md_table += \"|-\" + \"-|\".join([\"-\" * len(k) for k in single_values.keys()]) + \"-|\\n\"\n",
    "        md_table += \"| \" + \" | \".join(map(lambda v: str(round_value(v)), single_values.values())) + \" |\\n\"\n",
    "\n",
    "    return Markdown(md_table)\n",
    "\n",
    "\n",
    "def display_data_loss_table(data_dict, delta_sigma_pred, max_i):\n",
    "    \"\"\"\n",
    "    Erstellt eine Markdown-Tabelle zur übersichtlichen Darstellung von Datenverlust.\n",
    "    \n",
    "    Unterstützt sowohl Python-Listen als auch NumPy-Arrays.\n",
    "    \n",
    "    :param data_dict: Dictionary mit `sigma_t` und `delta_sigma` (Listen oder np.arrays)\n",
    "    :param delta_sigma_pred: Vorhergesagte Werte für `delta_sigma` (Liste oder np.array)\n",
    "    :param max_i: Anzahl der Werte, die in der Tabelle angezeigt werden sollen\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sicherstellen, dass `sigma_t` und `delta_sigma` existieren\n",
    "    if \"sigma_t\" not in data_dict or \"delta_sigma\" not in data_dict or delta_sigma_pred is None:\n",
    "        print(\"Fehler: `data_dict` oder `delta_sigma_pred` ist nicht korrekt definiert!\")\n",
    "        return\n",
    "\n",
    "    # Konvertiere alle Werte zu Listen (falls sie NumPy-Arrays sind)\n",
    "    def to_list(arr):\n",
    "        return arr.tolist() if isinstance(arr, np.ndarray) else arr\n",
    "\n",
    "    total_epsilon = to_list(data_dict[\"total_epsilon\"])\n",
    "    delta_epsilon = to_list(data_dict[\"delta_epsilon\"])\n",
    "    sigma_t = to_list(data_dict[\"sigma_t\"])\n",
    "    delta_sigma_true = to_list(data_dict[\"delta_sigma\"])\n",
    "    delta_sigma_pred = to_list(delta_sigma_pred.flatten())  # Falls `delta_sigma_pred` ein 2D-Array ist\n",
    "    \n",
    "    # Überprüfen, ob die Längen konsistent sind\n",
    "    min_len = min(len(total_epsilon), len(sigma_t), len(delta_epsilon), len(delta_sigma_true), len(delta_sigma_pred), max_i)\n",
    "\n",
    "    # Erstelle eine Tabelle für die übersichtliche Darstellung\n",
    "    data_loss_table = {\n",
    "        \"total_epsilon\" : list(total_epsilon[:min_len]), \n",
    "        \"delta_epsilon\" : list(delta_epsilon[:min_len]), \n",
    "        \"sigma_t\" : list(sigma_t[:min_len]), \n",
    "        \"True delta_sigma\": list(delta_sigma_true[:min_len]),\n",
    "        \"Predicted delta_sigma\": list(delta_sigma_pred[:min_len]),\n",
    "        \"Test-Loss (True - Predicted)\": list(np.round(np.array(delta_sigma_true[:min_len]) - np.array(delta_sigma_pred[:min_len]), 5))\n",
    "    }\n",
    "\n",
    "    # Markdown-Tabelle für bessere Darstellung in Jupyter\n",
    "    display(dict_to_markdown_table(data_loss_table, title=f\"Data-Loss bis sigma_{min_len-1}\", include_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dbada5-603e-41c4-a741-932497c15772",
   "metadata": {},
   "source": [
    "## Check for use of CONDA if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45429180-216e-482f-be87-1702b19c8890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0 -- Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    device_num = 0\n",
    "    print('No GPU available.')\n",
    "else:\n",
    "    device_num = torch.cuda.device_count()\n",
    "    print('Device:', device, '-- Number of devices:', device_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e8541-8716-411b-87f3-bcf5f94b71c9",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Just as people do not have to think again each time about the things they have already learned, it is also possible to teach neural networks to recall knowledge they were being taught. This is done in so-called Recurrent Neural Networks (RNNs) with loops inside, which allow information to be retained. Currently the most used architectures of RNNs are Long short-term memory (LSTM) networks. LSTMs are RNNs that overcome the problem of long-term dependencies and thus have achieved the most state-of-the-art results in this area. In this exercise we will look at how to use LSTMs to predict future values using time series data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee5e68d-54d7-40dd-bd8c-91f6c7349e3a",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80cbec0-a5d5-4310-88a0-db99ec043b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Oedometer:\n",
    "    def __init__(self, e_0: float = 1.00, C_c: float = 0.005, delta_epsilon: float = 0.0005, \n",
    "                 sigma_t: float = 1.00, max_n: int = 50, rand_epsilon:bool=False, **kwargs):\n",
    "        self.max_n = max_n\n",
    "\n",
    "        # Standardwerte als Listen setzen\n",
    "        self.e_0 = [e_0]\n",
    "        self.C_c = [C_c]\n",
    "        self.sigma_t = [sigma_t]\n",
    "        self.delta_epsilon = []\n",
    "        self.total_epsilon = [0]\n",
    "\n",
    "        # Initiale Listen für Berechnungen\n",
    "        self.sigma_t = [sigma_t]\n",
    "        self.delta_sigma = []\n",
    "        self.e_s = []\n",
    "        self.delta_epsilon = [delta_epsilon]\n",
    "        \n",
    "        # Dynamische Zuweisung von kwargs, falls vorhanden\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(self, key):  # Nur vorhandene Attribute setzen\n",
    "                setattr(self, key, [value])\n",
    "        \n",
    "        # Berechnungen durchführen\n",
    "        self.__calc_sigma_t_p1()\n",
    "\n",
    "        # Listenlängen anpassen\n",
    "        self.__adjust_list_lengths()\n",
    "        self.__calc_total_epsilon()\n",
    "\n",
    "    def __adjust_list_lengths(self):\n",
    "        \"\"\" Passt ALLE Listen-Attribute an `max_n` an. \"\"\"\n",
    "        attributes = ['e_0', 'C_c', 'delta_epsilon', 'sigma_t', 'sigma_t', 'delta_sigma', 'e_s']\n",
    "        for attr in attributes:\n",
    "            value_list = getattr(self, attr, [])\n",
    "            current_length = len(value_list)\n",
    "\n",
    "            if current_length > self.max_n:\n",
    "                setattr(self, attr, value_list[:self.max_n])  # Kürzen\n",
    "            elif current_length < self.max_n:\n",
    "                setattr(self, attr, value_list + [value_list[-1] if value_list else 0] * (self.max_n - current_length))  # Auffüllen\n",
    "    \n",
    "    def __calc_total_epsilon(self):\n",
    "        for i in range(len(self.delta_epsilon)-1):\n",
    "            self.total_epsilon.append(self.total_epsilon[i] + self.delta_epsilon[i])            \n",
    "    \n",
    "    def __calc_e_s(self, sigma_t):\n",
    "        \"\"\" Berechnet `e_s` aus `sigma_t`. \"\"\"\n",
    "        e_s = (1 + self.e_0[0]) / self.C_c[0] * sigma_t\n",
    "        self.e_s.append(e_s)\n",
    "        return e_s\n",
    "\n",
    "    def __calc_sigma_t_p1(self):\n",
    "        \"\"\" Berechnet `sigma_t` und `delta_sigma` für die nächsten Schritte. \"\"\"\n",
    "        for i in range(self.max_n):  # -1, weil sigma_t bereits gesetzt ist\n",
    "            e_s = self.__calc_e_s(self.sigma_t[i])\n",
    "            delta_sigma = e_s * self.delta_epsilon[0]\n",
    "            sigma = self.sigma_t[i] + delta_sigma\n",
    "            self.sigma_t.append(sigma)\n",
    "            self.delta_sigma.append(delta_sigma)\n",
    "\n",
    "def plot_input():\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data_dict_raw['sigma_t'], data_dict_raw['delta_sigma'], marker='o', linestyle='-', label='Sigma_0 = 1')\n",
    "    \n",
    "    # Formatting\n",
    "    plt.xlabel('sigma_t')\n",
    "    plt.ylabel('delta_simga')\n",
    "    plt.title('Sigma_0 in relation to Sigma_1')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9fb2661-f310-4b18-ab78-d819b4c9376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1000000\n",
    "\n",
    "oedo_para = {\n",
    "    'max_n': 1, \n",
    "    'e_0': 1.0, \n",
    "    'C_c': 0.005,  \n",
    "    'delta_epsilon': 0.0005, \n",
    "    'total_epsilon': 0,\n",
    "    'e_s': 400.0\n",
    "}\n",
    "\n",
    "# Vorbereitung Tensoren\n",
    "sigma_t = []\n",
    "delta_sigma = []\n",
    "delta_epsilon =  i * [.0005] # []\n",
    "for i in range(i):\n",
    "    oedo_para['sigma_t'] = r.randrange(0,500)\n",
    "    oedo = Oedometer(**oedo_para)\n",
    "    delta_sigma.append(round(oedo.delta_sigma[0], 2))\n",
    "    sigma_t.append(round(oedo.sigma_t[0], 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fdc2d5f-e8cf-4ffc-81cc-99daa881e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [['sigma_t'] + sigma_t, ['delta_sigma'] + delta_sigma, ['delta_epsilon'] + delta_epsilon]\n",
    "# import tabulate\n",
    "# table = tabulate.tabulate(data, tablefmt='html')\n",
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "931a932e-1761-4af9-bca2-ecb20ee57f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=50, num_layers=1, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM: input_size=2 (zwei Features pro Zeitschritt)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Linear Layer zur Ausgabe eines Werts pro Zeitschritt\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_size)\n",
    "        \n",
    "        # LSTM output: (batch_size, seq_length, hidden_size)\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # Ausgabe für jeden Zeitschritt (oder du könntest nur letzten Zeitschritt nehmen)\n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a5f72f-069b-4bd4-8ab0-eea81fb73942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([10, 100000, 2])\n",
      "Output shape: torch.Size([10, 100000, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sigma_t = torch.tensor(sigma_t)\n",
    "delta_epsilon = torch.tensor(delta_epsilon)\n",
    "delta_sigma = torch.tensor(delta_sigma)   \n",
    "\n",
    "N = len(sigma_t)\n",
    "\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "\n",
    "batch_size = 10     \n",
    "seq_length = N // batch_size \n",
    "\n",
    "# Auf ganze Anzahl kürzen:\n",
    "N_adjusted = batch_size * seq_length\n",
    "\n",
    "# Features zusammenpacken:\n",
    "inputs = torch.stack((sigma_t, delta_epsilon), dim=1)\n",
    "inputs = inputs[:N_adjusted]\n",
    "inputs = inputs.view(batch_size, seq_length, input_size)\n",
    "\n",
    "outputs = delta_sigma[:N_adjusted].view(batch_size, seq_length, output_size)\n",
    "\n",
    "print(\"Input shape:\", inputs.shape)\n",
    "print(\"Output shape:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3efe34e9-17ea-4248-bd89-7b02867e4890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3351.6643\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(input_size=input_size, hidden_size=10, num_layers=10, output_size=output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "outputs_pred = model(inputs.float())\n",
    "loss = criterion(outputs_pred, outputs.float())\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2509f46a-4114-472e-809a-57211294a294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.051155597\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "new_sigma_t = torch.tensor([1])      \n",
    "new_delta_epsilon = torch.tensor([.0005])\n",
    "with torch.no_grad():\n",
    "    new_input = torch.stack((new_sigma_t, new_delta_epsilon), dim=1).unsqueeze(0).float()\n",
    "    prediction = model(new_input)\n",
    "    prediction = prediction.squeeze().numpy()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a52cc-3aca-44bd-b33f-2043aaef2c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
