{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6c72d5-a9ea-4105-8ee1-ba8b63bdfb2c",
   "metadata": {},
   "source": [
    "# PINN with PINA\n",
    "## Load modules & Check PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf2a3e0-c8ef-4790-bab6-b26d4ac6c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# data processing\n",
    "import random as r\n",
    "from sys import exit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b4d96-913b-475a-aa5c-2d7ba7c996ba",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca778aa-d56d-4e7f-a094-7088dd12cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugger: Aktiviert\n",
    "debug_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b456b1-a8f1-4881-b330-a9cd9dd87be2",
   "metadata": {},
   "source": [
    "## Preloaded Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f76428-055a-42bd-9fb6-23809a78b3dc",
   "metadata": {},
   "source": [
    "## Check for use of CONDA if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb0bd7f-d96f-4e06-9c11-16091511a624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    device_num = 0\n",
    "    print('No GPU available.')\n",
    "else:\n",
    "    device_num = torch.cuda.device_count()\n",
    "    print('Device:', device, '-- Number of devices:', device_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b29402-afab-4b85-af61-5e576abe55fb",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Just as people do not have to think again each time about the things they have already learned, it is also possible to teach neural networks to recall knowledge they were being taught. This is done in so-called Recurrent Neural Networks (RNNs) with loops inside, which allow information to be retained. Currently the most used architectures of RNNs are Long short-term memory (LSTM) networks. LSTMs are RNNs that overcome the problem of long-term dependencies and thus have achieved the most state-of-the-art results in this area. In this exercise we will look at how to use LSTMs to predict future values using time series data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a337f98-69d9-4b33-90e8-9e3d5e761987",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7470b04a-d9d4-4b2b-9a99-33fdf5b6ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Oedometer:\n",
    "    def __init__(self, e_0: float = 1.00, C_c: float = 0.005, delta_epsilon: float = 0.0005, \n",
    "                 sigma_t: float = 1.00, max_n: int = 50, rand_epsilon:bool=False, **kwargs):\n",
    "        self.max_n = max_n\n",
    "\n",
    "        # Standardwerte als Listen setzen\n",
    "        self.e_0 = [e_0]\n",
    "        self.C_c = [C_c]\n",
    "        self.sigma_t = [sigma_t]\n",
    "        self.delta_epsilon = []\n",
    "        self.total_epsilon = [0]\n",
    "\n",
    "        # Initiale Listen für Berechnungen\n",
    "        self.sigma_t = [sigma_t]\n",
    "        self.delta_sigma = []\n",
    "        self.e_s = []\n",
    "        self.delta_epsilon = [delta_epsilon]\n",
    "        \n",
    "        # Dynamische Zuweisung von kwargs, falls vorhanden\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(self, key):  # Nur vorhandene Attribute setzen\n",
    "                setattr(self, key, [value])\n",
    "        \n",
    "        # Berechnungen durchführen\n",
    "        self.__calc_sigma_t_p1()\n",
    "\n",
    "        # Listenlängen anpassen\n",
    "        self.__adjust_list_lengths()\n",
    "        self.__calc_total_epsilon()\n",
    "\n",
    "    def __adjust_list_lengths(self):\n",
    "        \"\"\" Passt ALLE Listen-Attribute an `max_n` an. \"\"\"\n",
    "        attributes = ['e_0', 'C_c', 'delta_epsilon', 'sigma_t', 'sigma_t', 'delta_sigma', 'e_s']\n",
    "        for attr in attributes:\n",
    "            value_list = getattr(self, attr, [])\n",
    "            current_length = len(value_list)\n",
    "\n",
    "            if current_length > self.max_n:\n",
    "                setattr(self, attr, value_list[:self.max_n])  # Kürzen\n",
    "            elif current_length < self.max_n:\n",
    "                setattr(self, attr, value_list + [value_list[-1] if value_list else 0] * (self.max_n - current_length))  # Auffüllen\n",
    "    \n",
    "    def __calc_total_epsilon(self):\n",
    "        for i in range(len(self.delta_epsilon)-1):\n",
    "            self.total_epsilon.append(self.total_epsilon[i] + self.delta_epsilon[i])            \n",
    "    \n",
    "    def __calc_e_s(self, sigma_t):\n",
    "        \"\"\" Berechnet `e_s` aus `sigma_t`. \"\"\"\n",
    "        e_s = (1 + self.e_0[0]) / self.C_c[0] * sigma_t\n",
    "        self.e_s.append(e_s)\n",
    "        return e_s\n",
    "\n",
    "    def __calc_sigma_t_p1(self):\n",
    "        \"\"\" Berechnet `sigma_t` und `delta_sigma` für die nächsten Schritte. \"\"\"\n",
    "        for i in range(self.max_n):  # -1, weil sigma_t bereits gesetzt ist\n",
    "            e_s = self.__calc_e_s(self.sigma_t[i])\n",
    "            delta_sigma = e_s * self.delta_epsilon[0]\n",
    "            sigma = self.sigma_t[i] + delta_sigma\n",
    "            self.sigma_t.append(sigma)\n",
    "            self.delta_sigma.append(delta_sigma)\n",
    "\n",
    "def plot_input():\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data_dict_raw['sigma_t'], data_dict_raw['delta_sigma'], marker='o', linestyle='-', label='Sigma_0 = 1')\n",
    "    \n",
    "    # Formatting\n",
    "    plt.xlabel('sigma_t')\n",
    "    plt.ylabel('delta_simga')\n",
    "    plt.title('Sigma_0 in relation to Sigma_1')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d65dc0-6087-476e-bac1-8f50487dd87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Elemente delta_epsilon: 100\n",
      "Anzahl Elemente sigma_t: 100\n",
      "Anzahl Elemente delta_sigma: 100\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "oedo_para = {\n",
    "    'max_n': 100, \n",
    "    'e_0': 1.0, \n",
    "    'C_c': 0.005,   \n",
    "    'total_epsilon': 0,\n",
    "    'e_s': 400.0\n",
    "}\n",
    "\n",
    "# Vorbereitung Tensoren\n",
    "sigma_t = np.random.choice(range(1, 10000), size=i, replace=False)\n",
    "delta_sigma = []\n",
    "delta_epsilon = np.repeat(np.array(np.float64(0.0005)), oedo_para['max_n'])\n",
    "# delta_epsilon = np.random.uniform(0.0001, 0.001, size=i)\n",
    "\n",
    "oedo = Oedometer(**oedo_para)\n",
    "delta_sigma\n",
    "delta_sigma = list(oedo.delta_sigma)\n",
    "sigma_t = list(oedo.sigma_t)\n",
    "\n",
    "    \n",
    "print('Anzahl Elemente delta_epsilon: ' + str(len(delta_epsilon)))\n",
    "print('Anzahl Elemente sigma_t: ' + str(len(sigma_t)))\n",
    "print('Anzahl Elemente delta_sigma: ' + str(len(delta_sigma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "618642ef-70c4-4974-824a-aa3ed46f48d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>sigma_t      </td><td style=\"text-align: right;\">1     </td><td style=\"text-align: right;\">1.2   </td><td style=\"text-align: right;\">1.44  </td><td style=\"text-align: right;\">1.728 </td><td style=\"text-align: right;\">2.0736 </td><td style=\"text-align: right;\">2.48832 </td><td style=\"text-align: right;\">2.98598 </td><td style=\"text-align: right;\">3.58318 </td><td style=\"text-align: right;\">4.29982 </td><td style=\"text-align: right;\">5.15978</td><td style=\"text-align: right;\">6.19174</td><td style=\"text-align: right;\">7.43008</td><td style=\"text-align: right;\">8.9161 </td><td style=\"text-align: right;\">10.6993 </td><td style=\"text-align: right;\">12.8392 </td><td style=\"text-align: right;\">15.407 </td><td style=\"text-align: right;\">18.4884 </td><td style=\"text-align: right;\">22.1861 </td><td style=\"text-align: right;\">26.6233 </td><td style=\"text-align: right;\">31.948 </td><td style=\"text-align: right;\">38.3376 </td><td style=\"text-align: right;\">46.0051 </td><td style=\"text-align: right;\">55.2061</td><td style=\"text-align: right;\">66.2474</td><td style=\"text-align: right;\">79.4968</td><td style=\"text-align: right;\">95.3962</td><td style=\"text-align: right;\">114.475 </td><td style=\"text-align: right;\">137.371 </td><td style=\"text-align: right;\">164.845 </td><td style=\"text-align: right;\">197.814 </td><td style=\"text-align: right;\">237.376 </td><td style=\"text-align: right;\">284.852 </td><td style=\"text-align: right;\">341.822 </td><td style=\"text-align: right;\">410.186 </td><td style=\"text-align: right;\">492.224 </td><td style=\"text-align: right;\">590.668 </td><td style=\"text-align: right;\">708.802 </td><td style=\"text-align: right;\">850.562 </td><td style=\"text-align: right;\">1020.67  </td><td style=\"text-align: right;\">1224.81  </td><td style=\"text-align: right;\">1469.77  </td><td style=\"text-align: right;\">1763.73  </td><td style=\"text-align: right;\">2116.47  </td><td style=\"text-align: right;\">2539.77  </td><td style=\"text-align: right;\">3047.72  </td><td style=\"text-align: right;\">3657.26  </td><td style=\"text-align: right;\">4388.71  </td><td style=\"text-align: right;\">5266.46  </td><td style=\"text-align: right;\">6319.75  </td><td style=\"text-align: right;\">7583.7   </td><td style=\"text-align: right;\">9100.44  </td><td style=\"text-align: right;\">10920.5   </td><td style=\"text-align: right;\">13104.6   </td><td style=\"text-align: right;\">15725.6   </td><td style=\"text-align: right;\">18870.7   </td><td style=\"text-align: right;\">22644.8   </td><td style=\"text-align: right;\">27173.8   </td><td style=\"text-align: right;\">32608.5   </td><td style=\"text-align: right;\">39130.2   </td><td style=\"text-align: right;\">46956.3   </td><td style=\"text-align: right;\">56347.5   </td><td style=\"text-align: right;\">67617     </td><td style=\"text-align: right;\">81140.4   </td><td style=\"text-align: right;\">97368.5   </td><td style=\"text-align: right;\">116842     </td><td style=\"text-align: right;\">140211     </td><td style=\"text-align: right;\">168253     </td><td style=\"text-align: right;\">201903     </td><td style=\"text-align: right;\">242284     </td><td style=\"text-align: right;\">290741     </td><td style=\"text-align: right;\">348889     </td><td style=\"text-align: right;\">418667     </td><td style=\"text-align: right;\">502400     </td><td style=\"text-align: right;\">602880     </td><td style=\"text-align: right;\">723456     </td><td style=\"text-align: right;\">868147     </td><td style=\"text-align: right;\">     1.04178e+06</td><td style=\"text-align: right;\">     1.25013e+06</td><td style=\"text-align: right;\">     1.50016e+06</td><td style=\"text-align: right;\">     1.80019e+06</td><td style=\"text-align: right;\">     2.16023e+06</td><td style=\"text-align: right;\">     2.59227e+06</td><td style=\"text-align: right;\">     3.11073e+06</td><td style=\"text-align: right;\">     3.73287e+06</td><td style=\"text-align: right;\">     4.47945e+06</td><td style=\"text-align: right;\">5.37534e+06</td><td style=\"text-align: right;\">6.45041e+06</td><td style=\"text-align: right;\">7.74049e+06</td><td style=\"text-align: right;\">9.28859e+06</td><td style=\"text-align: right;\">1.11463e+07</td><td style=\"text-align: right;\">1.33756e+07</td><td style=\"text-align: right;\">1.60507e+07</td><td style=\"text-align: right;\">1.92608e+07</td><td style=\"text-align: right;\">2.3113e+07</td><td style=\"text-align: right;\">2.77356e+07</td><td style=\"text-align: right;\">3.32827e+07</td><td style=\"text-align: right;\">3.99392e+07</td><td style=\"text-align: right;\">4.79271e+07</td><td style=\"text-align: right;\">5.75125e+07</td><td style=\"text-align: right;\">6.9015e+07</td></tr>\n",
       "<tr><td>delta_sigma  </td><td style=\"text-align: right;\">0.2   </td><td style=\"text-align: right;\">0.24  </td><td style=\"text-align: right;\">0.288 </td><td style=\"text-align: right;\">0.3456</td><td style=\"text-align: right;\">0.41472</td><td style=\"text-align: right;\">0.497664</td><td style=\"text-align: right;\">0.597197</td><td style=\"text-align: right;\">0.716636</td><td style=\"text-align: right;\">0.859963</td><td style=\"text-align: right;\">1.03196</td><td style=\"text-align: right;\">1.23835</td><td style=\"text-align: right;\">1.48602</td><td style=\"text-align: right;\">1.78322</td><td style=\"text-align: right;\"> 2.13986</td><td style=\"text-align: right;\"> 2.56784</td><td style=\"text-align: right;\"> 3.0814</td><td style=\"text-align: right;\"> 3.69769</td><td style=\"text-align: right;\"> 4.43722</td><td style=\"text-align: right;\"> 5.32467</td><td style=\"text-align: right;\"> 6.3896</td><td style=\"text-align: right;\"> 7.66752</td><td style=\"text-align: right;\"> 9.20102</td><td style=\"text-align: right;\">11.0412</td><td style=\"text-align: right;\">13.2495</td><td style=\"text-align: right;\">15.8994</td><td style=\"text-align: right;\">19.0792</td><td style=\"text-align: right;\"> 22.8951</td><td style=\"text-align: right;\"> 27.4741</td><td style=\"text-align: right;\"> 32.9689</td><td style=\"text-align: right;\"> 39.5627</td><td style=\"text-align: right;\"> 47.4753</td><td style=\"text-align: right;\"> 56.9703</td><td style=\"text-align: right;\"> 68.3644</td><td style=\"text-align: right;\"> 82.0373</td><td style=\"text-align: right;\"> 98.4447</td><td style=\"text-align: right;\">118.134 </td><td style=\"text-align: right;\">141.76  </td><td style=\"text-align: right;\">170.112 </td><td style=\"text-align: right;\"> 204.135 </td><td style=\"text-align: right;\"> 244.962 </td><td style=\"text-align: right;\"> 293.954 </td><td style=\"text-align: right;\"> 352.745 </td><td style=\"text-align: right;\"> 423.294 </td><td style=\"text-align: right;\"> 507.953 </td><td style=\"text-align: right;\"> 609.544 </td><td style=\"text-align: right;\"> 731.452 </td><td style=\"text-align: right;\"> 877.743 </td><td style=\"text-align: right;\">1053.29  </td><td style=\"text-align: right;\">1263.95  </td><td style=\"text-align: right;\">1516.74  </td><td style=\"text-align: right;\">1820.09  </td><td style=\"text-align: right;\"> 2184.11  </td><td style=\"text-align: right;\"> 2620.93  </td><td style=\"text-align: right;\"> 3145.11  </td><td style=\"text-align: right;\"> 3774.13  </td><td style=\"text-align: right;\"> 4528.96  </td><td style=\"text-align: right;\"> 5434.75  </td><td style=\"text-align: right;\"> 6521.7   </td><td style=\"text-align: right;\"> 7826.04  </td><td style=\"text-align: right;\"> 9391.25  </td><td style=\"text-align: right;\">11269.5   </td><td style=\"text-align: right;\">13523.4   </td><td style=\"text-align: right;\">16228.1   </td><td style=\"text-align: right;\">19473.7   </td><td style=\"text-align: right;\"> 23368.4   </td><td style=\"text-align: right;\"> 28042.1   </td><td style=\"text-align: right;\"> 33650.6   </td><td style=\"text-align: right;\"> 40380.7   </td><td style=\"text-align: right;\"> 48456.8   </td><td style=\"text-align: right;\"> 58148.2   </td><td style=\"text-align: right;\"> 69777.8   </td><td style=\"text-align: right;\"> 83733.3   </td><td style=\"text-align: right;\">100480     </td><td style=\"text-align: right;\">120576     </td><td style=\"text-align: right;\">144691     </td><td style=\"text-align: right;\">173629     </td><td style=\"text-align: right;\">208355          </td><td style=\"text-align: right;\">250026          </td><td style=\"text-align: right;\">300032          </td><td style=\"text-align: right;\">360038          </td><td style=\"text-align: right;\">432046          </td><td style=\"text-align: right;\">518455          </td><td style=\"text-align: right;\">622146          </td><td style=\"text-align: right;\">746575          </td><td style=\"text-align: right;\">895890          </td><td style=\"text-align: right;\">1.07507e+06</td><td style=\"text-align: right;\">1.29008e+06</td><td style=\"text-align: right;\">1.5481e+06 </td><td style=\"text-align: right;\">1.85772e+06</td><td style=\"text-align: right;\">2.22926e+06</td><td style=\"text-align: right;\">2.67511e+06</td><td style=\"text-align: right;\">3.21014e+06</td><td style=\"text-align: right;\">3.85216e+06</td><td style=\"text-align: right;\">4.6226e+06</td><td style=\"text-align: right;\">5.54711e+06</td><td style=\"text-align: right;\">6.65654e+06</td><td style=\"text-align: right;\">7.98784e+06</td><td style=\"text-align: right;\">9.58541e+06</td><td style=\"text-align: right;\">1.15025e+07</td><td style=\"text-align: right;\">1.3803e+07</td></tr>\n",
       "<tr><td>delta_epsilon</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">0.0005 </td><td style=\"text-align: right;\">0.0005  </td><td style=\"text-align: right;\">0.0005  </td><td style=\"text-align: right;\">0.0005  </td><td style=\"text-align: right;\">0.0005  </td><td style=\"text-align: right;\">0.0005 </td><td style=\"text-align: right;\">0.0005 </td><td style=\"text-align: right;\">0.0005 </td><td style=\"text-align: right;\">0.0005 </td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005    </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<tbody>\\n<tr><td>sigma_t      </td><td style=\"text-align: right;\">1     </td><td style=\"text-align: right;\">1.2   </td><td style=\"text-align: right;\">1.44  </td><td style=\"text-align: right;\">1.728 </td><td style=\"text-align: right;\">2.0736 </td><td style=\"text-align: right;\">2.48832 </td><td style=\"text-align: right;\">2.98598 </td><td style=\"text-align: right;\">3.58318 </td><td style=\"text-align: right;\">4.29982 </td><td style=\"text-align: right;\">5.15978</td><td style=\"text-align: right;\">6.19174</td><td style=\"text-align: right;\">7.43008</td><td style=\"text-align: right;\">8.9161 </td><td style=\"text-align: right;\">10.6993 </td><td style=\"text-align: right;\">12.8392 </td><td style=\"text-align: right;\">15.407 </td><td style=\"text-align: right;\">18.4884 </td><td style=\"text-align: right;\">22.1861 </td><td style=\"text-align: right;\">26.6233 </td><td style=\"text-align: right;\">31.948 </td><td style=\"text-align: right;\">38.3376 </td><td style=\"text-align: right;\">46.0051 </td><td style=\"text-align: right;\">55.2061</td><td style=\"text-align: right;\">66.2474</td><td style=\"text-align: right;\">79.4968</td><td style=\"text-align: right;\">95.3962</td><td style=\"text-align: right;\">114.475 </td><td style=\"text-align: right;\">137.371 </td><td style=\"text-align: right;\">164.845 </td><td style=\"text-align: right;\">197.814 </td><td style=\"text-align: right;\">237.376 </td><td style=\"text-align: right;\">284.852 </td><td style=\"text-align: right;\">341.822 </td><td style=\"text-align: right;\">410.186 </td><td style=\"text-align: right;\">492.224 </td><td style=\"text-align: right;\">590.668 </td><td style=\"text-align: right;\">708.802 </td><td style=\"text-align: right;\">850.562 </td><td style=\"text-align: right;\">1020.67  </td><td style=\"text-align: right;\">1224.81  </td><td style=\"text-align: right;\">1469.77  </td><td style=\"text-align: right;\">1763.73  </td><td style=\"text-align: right;\">2116.47  </td><td style=\"text-align: right;\">2539.77  </td><td style=\"text-align: right;\">3047.72  </td><td style=\"text-align: right;\">3657.26  </td><td style=\"text-align: right;\">4388.71  </td><td style=\"text-align: right;\">5266.46  </td><td style=\"text-align: right;\">6319.75  </td><td style=\"text-align: right;\">7583.7   </td><td style=\"text-align: right;\">9100.44  </td><td style=\"text-align: right;\">10920.5   </td><td style=\"text-align: right;\">13104.6   </td><td style=\"text-align: right;\">15725.6   </td><td style=\"text-align: right;\">18870.7   </td><td style=\"text-align: right;\">22644.8   </td><td style=\"text-align: right;\">27173.8   </td><td style=\"text-align: right;\">32608.5   </td><td style=\"text-align: right;\">39130.2   </td><td style=\"text-align: right;\">46956.3   </td><td style=\"text-align: right;\">56347.5   </td><td style=\"text-align: right;\">67617     </td><td style=\"text-align: right;\">81140.4   </td><td style=\"text-align: right;\">97368.5   </td><td style=\"text-align: right;\">116842     </td><td style=\"text-align: right;\">140211     </td><td style=\"text-align: right;\">168253     </td><td style=\"text-align: right;\">201903     </td><td style=\"text-align: right;\">242284     </td><td style=\"text-align: right;\">290741     </td><td style=\"text-align: right;\">348889     </td><td style=\"text-align: right;\">418667     </td><td style=\"text-align: right;\">502400     </td><td style=\"text-align: right;\">602880     </td><td style=\"text-align: right;\">723456     </td><td style=\"text-align: right;\">868147     </td><td style=\"text-align: right;\">     1.04178e+06</td><td style=\"text-align: right;\">     1.25013e+06</td><td style=\"text-align: right;\">     1.50016e+06</td><td style=\"text-align: right;\">     1.80019e+06</td><td style=\"text-align: right;\">     2.16023e+06</td><td style=\"text-align: right;\">     2.59227e+06</td><td style=\"text-align: right;\">     3.11073e+06</td><td style=\"text-align: right;\">     3.73287e+06</td><td style=\"text-align: right;\">     4.47945e+06</td><td style=\"text-align: right;\">5.37534e+06</td><td style=\"text-align: right;\">6.45041e+06</td><td style=\"text-align: right;\">7.74049e+06</td><td style=\"text-align: right;\">9.28859e+06</td><td style=\"text-align: right;\">1.11463e+07</td><td style=\"text-align: right;\">1.33756e+07</td><td style=\"text-align: right;\">1.60507e+07</td><td style=\"text-align: right;\">1.92608e+07</td><td style=\"text-align: right;\">2.3113e+07</td><td style=\"text-align: right;\">2.77356e+07</td><td style=\"text-align: right;\">3.32827e+07</td><td style=\"text-align: right;\">3.99392e+07</td><td style=\"text-align: right;\">4.79271e+07</td><td style=\"text-align: right;\">5.75125e+07</td><td style=\"text-align: right;\">6.9015e+07</td></tr>\\n<tr><td>delta_sigma  </td><td style=\"text-align: right;\">0.2   </td><td style=\"text-align: right;\">0.24  </td><td style=\"text-align: right;\">0.288 </td><td style=\"text-align: right;\">0.3456</td><td style=\"text-align: right;\">0.41472</td><td style=\"text-align: right;\">0.497664</td><td style=\"text-align: right;\">0.597197</td><td style=\"text-align: right;\">0.716636</td><td style=\"text-align: right;\">0.859963</td><td style=\"text-align: right;\">1.03196</td><td style=\"text-align: right;\">1.23835</td><td style=\"text-align: right;\">1.48602</td><td style=\"text-align: right;\">1.78322</td><td style=\"text-align: right;\"> 2.13986</td><td style=\"text-align: right;\"> 2.56784</td><td style=\"text-align: right;\"> 3.0814</td><td style=\"text-align: right;\"> 3.69769</td><td style=\"text-align: right;\"> 4.43722</td><td style=\"text-align: right;\"> 5.32467</td><td style=\"text-align: right;\"> 6.3896</td><td style=\"text-align: right;\"> 7.66752</td><td style=\"text-align: right;\"> 9.20102</td><td style=\"text-align: right;\">11.0412</td><td style=\"text-align: right;\">13.2495</td><td style=\"text-align: right;\">15.8994</td><td style=\"text-align: right;\">19.0792</td><td style=\"text-align: right;\"> 22.8951</td><td style=\"text-align: right;\"> 27.4741</td><td style=\"text-align: right;\"> 32.9689</td><td style=\"text-align: right;\"> 39.5627</td><td style=\"text-align: right;\"> 47.4753</td><td style=\"text-align: right;\"> 56.9703</td><td style=\"text-align: right;\"> 68.3644</td><td style=\"text-align: right;\"> 82.0373</td><td style=\"text-align: right;\"> 98.4447</td><td style=\"text-align: right;\">118.134 </td><td style=\"text-align: right;\">141.76  </td><td style=\"text-align: right;\">170.112 </td><td style=\"text-align: right;\"> 204.135 </td><td style=\"text-align: right;\"> 244.962 </td><td style=\"text-align: right;\"> 293.954 </td><td style=\"text-align: right;\"> 352.745 </td><td style=\"text-align: right;\"> 423.294 </td><td style=\"text-align: right;\"> 507.953 </td><td style=\"text-align: right;\"> 609.544 </td><td style=\"text-align: right;\"> 731.452 </td><td style=\"text-align: right;\"> 877.743 </td><td style=\"text-align: right;\">1053.29  </td><td style=\"text-align: right;\">1263.95  </td><td style=\"text-align: right;\">1516.74  </td><td style=\"text-align: right;\">1820.09  </td><td style=\"text-align: right;\"> 2184.11  </td><td style=\"text-align: right;\"> 2620.93  </td><td style=\"text-align: right;\"> 3145.11  </td><td style=\"text-align: right;\"> 3774.13  </td><td style=\"text-align: right;\"> 4528.96  </td><td style=\"text-align: right;\"> 5434.75  </td><td style=\"text-align: right;\"> 6521.7   </td><td style=\"text-align: right;\"> 7826.04  </td><td style=\"text-align: right;\"> 9391.25  </td><td style=\"text-align: right;\">11269.5   </td><td style=\"text-align: right;\">13523.4   </td><td style=\"text-align: right;\">16228.1   </td><td style=\"text-align: right;\">19473.7   </td><td style=\"text-align: right;\"> 23368.4   </td><td style=\"text-align: right;\"> 28042.1   </td><td style=\"text-align: right;\"> 33650.6   </td><td style=\"text-align: right;\"> 40380.7   </td><td style=\"text-align: right;\"> 48456.8   </td><td style=\"text-align: right;\"> 58148.2   </td><td style=\"text-align: right;\"> 69777.8   </td><td style=\"text-align: right;\"> 83733.3   </td><td style=\"text-align: right;\">100480     </td><td style=\"text-align: right;\">120576     </td><td style=\"text-align: right;\">144691     </td><td style=\"text-align: right;\">173629     </td><td style=\"text-align: right;\">208355          </td><td style=\"text-align: right;\">250026          </td><td style=\"text-align: right;\">300032          </td><td style=\"text-align: right;\">360038          </td><td style=\"text-align: right;\">432046          </td><td style=\"text-align: right;\">518455          </td><td style=\"text-align: right;\">622146          </td><td style=\"text-align: right;\">746575          </td><td style=\"text-align: right;\">895890          </td><td style=\"text-align: right;\">1.07507e+06</td><td style=\"text-align: right;\">1.29008e+06</td><td style=\"text-align: right;\">1.5481e+06 </td><td style=\"text-align: right;\">1.85772e+06</td><td style=\"text-align: right;\">2.22926e+06</td><td style=\"text-align: right;\">2.67511e+06</td><td style=\"text-align: right;\">3.21014e+06</td><td style=\"text-align: right;\">3.85216e+06</td><td style=\"text-align: right;\">4.6226e+06</td><td style=\"text-align: right;\">5.54711e+06</td><td style=\"text-align: right;\">6.65654e+06</td><td style=\"text-align: right;\">7.98784e+06</td><td style=\"text-align: right;\">9.58541e+06</td><td style=\"text-align: right;\">1.15025e+07</td><td style=\"text-align: right;\">1.3803e+07</td></tr>\\n<tr><td>delta_epsilon</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">0.0005</td><td style=\"text-align: right;\">0.0005 </td><td style=\"text-align: right;\">0.0005  </td><td style=\"text-align: right;\">0.0005  </td><td style=\"text-align: right;\">0.0005  </td><td style=\"text-align: right;\">0.0005  </td><td style=\"text-align: right;\">0.0005 </td><td style=\"text-align: right;\">0.0005 </td><td style=\"text-align: right;\">0.0005 </td><td style=\"text-align: right;\">0.0005 </td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005 </td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\"> 0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">  0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">   0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">    0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005</td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">     0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005    </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005     </td><td style=\"text-align: right;\">0.0005    </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [['sigma_t'] + sigma_t, ['delta_sigma'] + delta_sigma, ['delta_epsilon'] + delta_epsilon.tolist()]\n",
    "import tabulate\n",
    "table = tabulate.tabulate(data, tablefmt='html')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ce75e5-2b3c-41c1-af01-6c10a69b72e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input1</th>\n",
       "      <th>input2</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.897922e-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.897922e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.375428e-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.375428e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.054843e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.054843e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.555604e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.555604e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4.822531e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.822531e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.787037e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.787037e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6.944444e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.944444e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>8.333333e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.333333e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          input1  input2        output\n",
       "0   0.000000e+00     0.0  0.000000e+00\n",
       "1   2.897922e-09     0.0  2.897922e-09\n",
       "2   6.375428e-09     0.0  6.375428e-09\n",
       "3   1.054843e-08     0.0  1.054843e-08\n",
       "4   1.555604e-08     0.0  1.555604e-08\n",
       "..           ...     ...           ...\n",
       "95  4.822531e-01     0.0  4.822531e-01\n",
       "96  5.787037e-01     0.0  5.787037e-01\n",
       "97  6.944444e-01     0.0  6.944444e-01\n",
       "98  8.333333e-01     0.0  8.333333e-01\n",
       "99  1.000000e+00     0.0  1.000000e+00\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = {\n",
    "    'input1': sigma_t,\n",
    "    'input2': delta_epsilon,\n",
    "    'output': delta_sigma\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalisierung\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Zurück in DataFrame\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9fd43e-f63d-45b8-9b94-e85b1363fd36",
   "metadata": {},
   "source": [
    "## Auszug aus Tutorial 6\n",
    "\n",
    "    # Create sequences\n",
    "    # Function to create sequences of input (passenger of train_window months) and ...\n",
    "    # ... output passenger of train_window+1 month\n",
    "    \n",
    "    def create_inout_sequences(input_data, tw):\n",
    "        inout_seq = []\n",
    "        L = len(input_data)\n",
    "        for i in range(L-tw):\n",
    "            train_seq = input_data[i:i+tw]\n",
    "            train_label = input_data[i+tw:i+tw+1]\n",
    "            inout_seq.append((train_seq ,train_<label))\n",
    "        return inout_seq\n",
    "    \n",
    "    # Set an appropiate Train window\n",
    "    # TODO start**\n",
    "    train_window = 24\n",
    "    \n",
    "    # Create sequences \n",
    "    trva_inout_seq = create_inout_sequences(trva_data_normalized, train_window)\n",
    "    \n",
    "    # Print the first 3 sequences\n",
    "    print(trva_inout_seq[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c6ca71-e3a8-4e33-8cd5-caddf6dd06bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 2])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "lookback=1\n",
    "\n",
    "def create_dataset(df, lookback=1):\n",
    "    \"\"\"\n",
    "    Erzeugt die Eingabe- und Ziel-Datensätze für ein LSTM.\n",
    "    \n",
    "    lookback definiert hier, wie viele vergangene Zeitschritte\n",
    "    (Train Window / Sequences im Tutorial) das Modell als Input\n",
    "    für jede Vorhersage erhält. Dadurch entsteht die nötige\n",
    "    3D-Form (batch_size, seq_length, input_size), die ein LSTM erwartet,\n",
    "    um zeitliche Abhängigkeiten zu lernen.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback):\n",
    "        # Wir nehmen hier 'lookback' Zeilen als eine Sequenz,\n",
    "        # damit das LSTM-Modell weiß, wie weit es in die Vergangenheit\n",
    "        # zurückschauen muss. Ohne diese Sequenz-Dimension (seq_length)\n",
    "        # wäre die Eingabe nur eine 2D-Matrix und das LSTM könnte\n",
    "        # keine zeitlichen Muster erfassen.\n",
    "        seq = df.iloc[i:i+lookback, :-1].values\n",
    "        X.append(seq)\n",
    "        \n",
    "        # Zielwert: der Wert direkt nach dieser Sequenz\n",
    "        y.append(df.iloc[i+lookback, -1])\n",
    "    \n",
    "    # Rückgabe als Tensor mit Form (batch_size, seq_length, input_size) und (batch_size,)\n",
    "    return torch.tensor(np.array(X), dtype=torch.float32), \\\n",
    "           torch.tensor(np.array(y), dtype=torch.float32)\n",
    "\n",
    "X, y = create_dataset(df_scaled, lookback)\n",
    "X_input_all = df_scaled.iloc[lookback:, :-1].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Falls lookback=1, wäre X anfangs (5000, 1, 2).\n",
    "# Mit squeeze() könnte man die mittlere Dimension entfernen,\n",
    "# aber für LSTM brauchen wir sie genau so:\n",
    "X = X.squeeze()\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104c8184-fa10-47c6-8b9b-9acf80aedc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_train: torch.Size([66, 2]) y_train: torch.Size([66, 1]) X_input_train: (66, 2)\n",
      "  X_test:  torch.Size([33, 2]) y_test:  torch.Size([33, 1]) X_input_test:  (33, 2)\n"
     ]
    }
   ],
   "source": [
    "# --- Aufteilen in Training (2/3) und Test (1/3) ---\n",
    "train_size = int(X.shape[0] * 2/3)\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size].view(-1, 1)  # als (batch_size, 1)\n",
    "X_test  = X[train_size:]\n",
    "y_test  = y[train_size:].view(-1, 1)\n",
    "\n",
    "X_input_train = X_input_all[:train_size]\n",
    "X_input_test  = X_input_all[train_size:]\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", X_train.shape, \"y_train:\", y_train.shape, \"X_input_train:\", X_input_train.shape)\n",
    "print(\"  X_test: \", X_test.shape,  \"y_test: \", y_test.shape,  \"X_input_test: \",  X_input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "981291e7-ccbe-4bd5-ba3e-0ecfddc8e744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‼️Geladene Input Variablen:  ['sigma_t', 'delta_epsilon']\n",
      "‼️Geladene Output Variablen:  ['delta_sigma']\n",
      "‼️Input points: {'data': LabelTensor([[0.0000e+00, 0.0000e+00],\n",
      "             [2.8979e-09, 0.0000e+00],\n",
      "             [6.3754e-09, 0.0000e+00],\n",
      "             [1.0548e-08, 0.0000e+00],\n",
      "             [1.5556e-08, 0.0000e+00],\n",
      "             [2.1565e-08, 0.0000e+00],\n",
      "             [2.8776e-08, 0.0000e+00],\n",
      "             [3.7429e-08, 0.0000e+00],\n",
      "             [4.7813e-08, 0.0000e+00],\n",
      "             [6.0274e-08, 0.0000e+00],\n",
      "             [7.5226e-08, 0.0000e+00],\n",
      "             [9.3169e-08, 0.0000e+00],\n",
      "             [1.1470e-07, 0.0000e+00],\n",
      "             [1.4054e-07, 0.0000e+00],\n",
      "             [1.7155e-07, 0.0000e+00],\n",
      "             [2.0875e-07, 0.0000e+00],\n",
      "             [2.5340e-07, 0.0000e+00],\n",
      "             [3.0698e-07, 0.0000e+00],\n",
      "             [3.7127e-07, 0.0000e+00],\n",
      "             [4.4842e-07, 0.0000e+00],\n",
      "             [5.4101e-07, 0.0000e+00],\n",
      "             [6.5211e-07, 0.0000e+00],\n",
      "             [7.8543e-07, 0.0000e+00],\n",
      "             [9.4541e-07, 0.0000e+00],\n",
      "             [1.1374e-06, 0.0000e+00],\n",
      "             [1.3678e-06, 0.0000e+00],\n",
      "             [1.6442e-06, 0.0000e+00],\n",
      "             [1.9760e-06, 0.0000e+00],\n",
      "             [2.3740e-06, 0.0000e+00],\n",
      "             [2.8518e-06, 0.0000e+00],\n",
      "             [3.4250e-06, 0.0000e+00],\n",
      "             [4.1129e-06, 0.0000e+00],\n",
      "             [4.9384e-06, 0.0000e+00],\n",
      "             [5.9289e-06, 0.0000e+00],\n",
      "             [7.1176e-06, 0.0000e+00],\n",
      "             [8.5441e-06, 0.0000e+00],\n",
      "             [1.0256e-05, 0.0000e+00],\n",
      "             [1.2310e-05, 0.0000e+00],\n",
      "             [1.4775e-05, 0.0000e+00],\n",
      "             [1.7733e-05, 0.0000e+00],\n",
      "             [2.1282e-05, 0.0000e+00],\n",
      "             [2.5541e-05, 0.0000e+00],\n",
      "             [3.0652e-05, 0.0000e+00],\n",
      "             [3.6786e-05, 0.0000e+00],\n",
      "             [4.4146e-05, 0.0000e+00],\n",
      "             [5.2978e-05, 0.0000e+00],\n",
      "             [6.3576e-05, 0.0000e+00],\n",
      "             [7.6294e-05, 0.0000e+00],\n",
      "             [9.1556e-05, 0.0000e+00],\n",
      "             [1.0987e-04, 0.0000e+00],\n",
      "             [1.3185e-04, 0.0000e+00],\n",
      "             [1.5822e-04, 0.0000e+00],\n",
      "             [1.8987e-04, 0.0000e+00],\n",
      "             [2.2784e-04, 0.0000e+00],\n",
      "             [2.7341e-04, 0.0000e+00],\n",
      "             [3.2810e-04, 0.0000e+00],\n",
      "             [3.9372e-04, 0.0000e+00],\n",
      "             [4.7247e-04, 0.0000e+00],\n",
      "             [5.6697e-04, 0.0000e+00],\n",
      "             [6.8036e-04, 0.0000e+00],\n",
      "             [8.1644e-04, 0.0000e+00],\n",
      "             [9.7973e-04, 0.0000e+00],\n",
      "             [1.1757e-03, 0.0000e+00],\n",
      "             [1.4108e-03, 0.0000e+00],\n",
      "             [1.6930e-03, 0.0000e+00],\n",
      "             [2.0316e-03, 0.0000e+00]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\Documents\\git_projects\\pina_oedometer\\venv\\Lib\\site-packages\\pina\\geometry\\__init__.py: DeprecationWarning: 'pina.geometry' is deprecated and will be removed in future versions. Please use 'pina.domain' instead. Location moved to DomainInferface object.\n"
     ]
    }
   ],
   "source": [
    "# PINA NN Model\n",
    "from pina.problem import AbstractProblem\n",
    "from pina.geometry import CartesianDomain\n",
    "from pina import Condition\n",
    "from pina.utils import LabelTensor\n",
    "\n",
    "input_points_combined = LabelTensor(X_train, ['sigma_t', 'delta_epsilon'])\n",
    "delta_sigma_train = LabelTensor(y_train, ['delta_sigma'])\n",
    "input_conditions = {'data': Condition(input=input_points_combined, target=delta_sigma_train),}\n",
    "\n",
    "\n",
    "class SimpleODE(AbstractProblem):\n",
    "\n",
    "    # Definition der Eingabe- und Ausgabevariablen basierend auf LabelTensor\n",
    "    input_variables = input_points_combined.labels\n",
    "    output_variables = delta_sigma_train.labels\n",
    "\n",
    "    # Wertebereich\n",
    "    domain = CartesianDomain({'sigma_t': [0, 1], 'delta_epsilon': [0, 1]})  # Wertebereich immer definieren!\n",
    "\n",
    "    # Definition der Randbedingungen und (hier: nur) vorberechnetet Punkte\n",
    "    conditions = input_conditions\n",
    "\n",
    "    output_pts=delta_sigma_train\n",
    "\n",
    "    # Methode zur Definition der \"wahren Lösung\" des Problems\n",
    "    def truth_solution(self, pts):\n",
    "        return torch.exp(pts.extract(['sigma_t']))\n",
    "\n",
    "# Problem-Instanz erzeugen\n",
    "problem = SimpleODE()\n",
    "\n",
    "\n",
    "\n",
    "if debug_mode:\n",
    "    # Debugging-Ausgaben\n",
    "    print(\"‼️Geladene Input Variablen: \", problem.input_variables)\n",
    "    print(\"‼️Geladene Output Variablen: \", problem.output_variables)\n",
    "    print('‼️Input points:', problem.input_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9043ce0-2973-4ded-b4ab-78b0b9eee054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T08:30:33.410973Z",
     "start_time": "2025-03-12T08:30:32.818773Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging Info:\n",
      "‼️Länge der Eingabepunkte (input_pts): 1\n",
      "‼️Länge der Ausgabepunkte (output_pts): 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukas\\Documents\\git_projects\\pina_oedometer\\venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "C:\\Users\\lukas\\Documents\\git_projects\\pina_oedometer\\venv\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|█| 7/7 [00:00<00:00, 35.45it/s, v_num=49, data_loss_step=4.22e-8, train_loss_step=4.22e-8, data_loss_epo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|█| 7/7 [00:00<00:00, 30.66it/s, v_num=49, data_loss_step=4.22e-8, train_loss_step=4.22e-8, data_loss_epo\n",
      "\n",
      "Finale Loss Werte\n"
     ]
    }
   ],
   "source": [
    "from pina import Trainer\n",
    "from pina.solver import PINN\n",
    "from pina.model import FeedForward\n",
    "from pina.callback import MetricTracker\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger  # Import TensorBoard Logger\n",
    "\n",
    "if debug_mode:\n",
    "    print('Debugging Info:')\n",
    "    # Überprüfen der Größe der Eingabepunkte und Ausgabepunkte\n",
    "    print(\"‼️Länge der Eingabepunkte (input_pts):\", len(problem.input_pts))\n",
    "    print(\"‼️Länge der Ausgabepunkte (output_pts):\", len(problem.output_pts))\n",
    "\n",
    "# Model erstellen\n",
    "model = FeedForward(\n",
    "    layers=[50, 50, 50],\n",
    "    func=torch.nn.ReLU,\n",
    "    output_dimensions=len(problem.output_variables),\n",
    "    input_dimensions=len(problem.input_variables)\n",
    ")\n",
    "\n",
    "# PINN-Objekt erstellen\n",
    "pinn = PINN(problem, model)\n",
    "\n",
    "\n",
    "# Trainer erstellen mit TensorBoard-Logger\n",
    "trainer = Trainer(\n",
    "    solver=pinn,\n",
    "    max_epochs=100,\n",
    "    callbacks=[MetricTracker()],\n",
    "    batch_size=10,\n",
    "    accelerator='cpu',\n",
    "    enable_model_summary=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "print('\\nFinale Loss Werte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f11f034-df9f-425d-aee0-d0a25945d3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "trainer_metrics = trainer.callbacks[0].metrics\n",
    "loss = trainer_metrics[\"train_loss_epoch\"]\n",
    "epochs = range(len(loss))\n",
    "plt.plot(epochs, loss.cpu())\n",
    "# plotting\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e5a6697-81da-44eb-836a-e32e6a8f936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def plot_prediction_vs_true_solution(pinn, data_dict, graph_folder, img_visual_prediction_vs_truesolution, \n",
    "                                     img_extensions, y_axis='delta_sigma', max_i=20, plot_type=\"line\"):\n",
    "    \"\"\"\n",
    "    Erstellt und speichert eine Vorhersage- vs. True-Solution-Grafik für ein gegebenes PINN-Modell.\n",
    "\n",
    "    :param pinn: Das trainierte PINN-Modell zur Vorhersage von delta_sigma\n",
    "    :param data_dict: Dictionary mit den Eingabe- und wahren Ausgabe-Daten\n",
    "    :param graph_folder: Ordner, in dem das Bild gespeichert wird\n",
    "    :param img_visual_prediction_vs_truesolution: Dateiname der gespeicherten Grafik (ohne Erweiterung)\n",
    "    :param img_extensions: Dateiformat der gespeicherten Grafik (z.B. '.png' oder '.jpg')\n",
    "    :param max_i: Anzahl der Datenpunkte, die im Plot gezeigt werden sollen (Default: 20)\n",
    "    :param delta_epsilon: Wert für delta_epsilon, um ihn im Titel anzuzeigen (optional)\n",
    "    :param plot_type: Art der Darstellung - \"line\" für Linienplot, \"scatter\" für Punktplot (Default: \"line\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Überprüfen, ob die notwendigen Keys vorhanden sind\n",
    "    if \"sigma_t\" not in data_dict or y_axis not in data_dict:\n",
    "        print(f\"Fehler: sigma_t oder y_axis fehlen im data_dict!\")\n",
    "        return\n",
    "\n",
    "    # Eingabedaten für das Modell vorbereiten\n",
    "    input_data = LabelTensor(torch.tensor(\n",
    "        np.column_stack((data_dict['sigma_t'], data_dict['delta_epsilon'])), \n",
    "        dtype=torch.float), ['sigma_t', 'delta_epsilon'])\n",
    "\n",
    "    # Vorhersage berechnen\n",
    "    sigma_t_pred = pinn(input_data).detach().numpy()\n",
    "\n",
    "    # Plot erstellen\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    y_vals = data_dict[y_axis][0:max_i]\n",
    "    x_true = data_dict['delta_sigma'][0:max_i]\n",
    "    x_pred = sigma_t_pred[0:max_i]\n",
    "\n",
    "    if plot_type == \"line\":\n",
    "        plt.plot(x_true, y_vals, label=\"True Solution (delta_sigma)\", linestyle='dashed', color='blue')\n",
    "        plt.plot(x_pred, y_vals, label=\"NN Prediction (delta_sigma)\", linestyle='solid', color='red')\n",
    "    elif plot_type == \"scatter\":\n",
    "        plt.scatter(x_true, y_vals, label=\"True Solution (delta_sigma)\", color='blue', marker='o')\n",
    "        plt.scatter(x_pred, y_vals, label=\"NN Prediction (delta_sigma)\", color='red', marker='x')\n",
    "\n",
    "    plt.xlabel(\"delta_sigma\")\n",
    "    plt.ylabel(y_axis)\n",
    "    plt.title(f\"Prediction vs. True Solution (max_i={max_i-1})\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Bild speichern\n",
    "    img_path = f'./{graph_folder}/{img_visual_prediction_vs_truesolution}{img_extensions}'\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()  # Verhindert doppelte Darstellung\n",
    "\n",
    "    # Markdown-Ausgabe in Jupyter Notebook\n",
    "    display(Markdown(f'![Prediction vs True Solution]({img_path})<br>**Hinweis:** Datenpunkte liegen sehr nahe beieinander.'))\n",
    "\n",
    "def display_data_loss_table(data_dict, delta_sigma_pred, max_i):\n",
    "    \"\"\"\n",
    "    Erstellt eine Markdown-Tabelle zur übersichtlichen Darstellung von Datenverlust.\n",
    "    \n",
    "    Unterstützt sowohl Python-Listen als auch NumPy-Arrays.\n",
    "    \n",
    "    :param data_dict: Dictionary mit `sigma_t` und `delta_sigma` (Listen oder np.arrays)\n",
    "    :param delta_sigma_pred: Vorhergesagte Werte für `delta_sigma` (Liste oder np.array)\n",
    "    :param max_i: Anzahl der Werte, die in der Tabelle angezeigt werden sollen\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sicherstellen, dass `sigma_t` und `delta_sigma` existieren\n",
    "    if \"sigma_t\" not in data_dict or \"delta_sigma\" not in data_dict or delta_sigma_pred is None:\n",
    "        print(\"Fehler: `data_dict` oder `delta_sigma_pred` ist nicht korrekt definiert!\")\n",
    "        return\n",
    "\n",
    "    # Konvertiere alle Werte zu Listen (falls sie NumPy-Arrays sind)\n",
    "    def to_list(arr):\n",
    "        return arr.tolist() if isinstance(arr, np.ndarray) else arr\n",
    "\n",
    "    delta_epsilon = to_list(data_dict[\"delta_epsilon\"])\n",
    "    sigma_t = to_list(data_dict[\"sigma_t\"])\n",
    "    delta_sigma_true = to_list(data_dict[\"delta_sigma\"])\n",
    "    delta_sigma_pred = to_list(delta_sigma_pred.flatten())  # Falls `delta_sigma_pred` ein 2D-Array ist\n",
    "    \n",
    "    # Überprüfen, ob die Längen konsistent sind\n",
    "    min_len = min(len(sigma_t), len(delta_epsilon), len(delta_sigma_true), len(delta_sigma_pred), max_i)\n",
    "\n",
    "    # Erstelle eine Tabelle für die übersichtliche Darstellung\n",
    "    data_loss_table = {\n",
    "        \"delta_epsilon\" : list(delta_epsilon[:min_len]), \n",
    "        \"sigma_t\" : list(sigma_t[:min_len]), \n",
    "        \"True delta_sigma\": list(delta_sigma_true[:min_len]),\n",
    "        \"Predicted delta_sigma\": list(delta_sigma_pred[:min_len]),\n",
    "        \"Test-Loss (True - Predicted)\": list(np.round(np.array(delta_sigma_true[:min_len]) - np.array(delta_sigma_pred[:min_len]), 5))\n",
    "    }\n",
    "\n",
    "    # Markdown-Tabelle für bessere Darstellung in Jupyter\n",
    "    display(dict_to_markdown_table(data_loss_table, title=f\"Data-Loss bis sigma_{min_len-1}\", include_index=True))\n",
    "\n",
    "def dict_to_markdown_table(data: dict, title: str = \"Datenübersicht\", include_index: bool = True, round_digits: int = 4):\n",
    "    \"\"\"\n",
    "    Wandelt ein Dictionary mit Listenwerten in eine Markdown-Tabelle für Jupyter Notebooks um.\n",
    "    \n",
    "    - Schlüssel werden als Header genutzt\n",
    "    - Erste Spalte ist ein Index, falls `include_index=True`\n",
    "    - Einzelwerte werden als separate Tabelle unterhalb dargestellt\n",
    "    - Zahlenwerte werden auf eine einstellbare Anzahl an Nachkommastellen gerundet\n",
    "\n",
    "    :param data: Dictionary mit Key-Value-Paaren\n",
    "    :param title: Überschrift für die Tabelle\n",
    "    :param include_index: Falls True, wird eine Index-Spalte erstellt\n",
    "    :param round_digits: Anzahl der Nachkommastellen, auf die Werte gerundet werden sollen\n",
    "    :return: Markdown-String zur Anzeige in Jupyter\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hilfsfunktion zum Runden von Zahlen\n",
    "    def round_value(val):\n",
    "        if isinstance(val, (int, float)):\n",
    "            return round(val, round_digits)\n",
    "        return val\n",
    "\n",
    "    # Listen und einzelne Werte trennen\n",
    "    list_data = {k: v for k, v in data.items() if isinstance(v, list)}\n",
    "    single_values = {k: v for k, v in data.items() if not isinstance(v, list)}\n",
    "\n",
    "    # Falls es Listen gibt, erstelle eine Tabelle mit Index\n",
    "    if list_data:\n",
    "        max_len = max(len(v) for v in list_data.values())  # Längste Liste bestimmen\n",
    "\n",
    "        # Tabellenkopf\n",
    "        md_table = f\"### {title}\\n\\n\"\n",
    "        md_table += \"| \" + (\"Index | \" if include_index else \"\") + \" | \".join(list_data.keys()) + \" |\\n\"\n",
    "        md_table += \"|-\" + (\"-|\" if include_index else \"\") + \"-|\".join([\"-\" * len(k) for k in list_data.keys()]) + \"-|\\n\"\n",
    "\n",
    "        # Datenzeilen\n",
    "        for i in range(max_len):\n",
    "            row = [str(i)] if include_index else []  # Index hinzufügen (optional)\n",
    "            for key in list_data:\n",
    "                if i < len(list_data[key]):\n",
    "                    row.append(str(round_value(list_data[key][i])))\n",
    "                else:\n",
    "                    row.append(\"\")  # Leere Werte für ungleich lange Listen\n",
    "            md_table += \"| \" + \" | \".join(row) + \" |\\n\"\n",
    "    \n",
    "    else:\n",
    "        md_table = \"\"\n",
    "\n",
    "    # Einzelwerte als extra Tabelle darstellen\n",
    "    if single_values:\n",
    "        md_table += \"\\n\\n#### Einzelwerte\\n\\n\"\n",
    "        md_table += \"| \" + \" | \".join(single_values.keys()) + \" |\\n\"\n",
    "        md_table += \"|-\" + \"-|\".join([\"-\" * len(k) for k in single_values.keys()]) + \"-|\\n\"\n",
    "        md_table += \"| \" + \" | \".join(map(lambda v: str(round_value(v)), single_values.values())) + \" |\\n\"\n",
    "\n",
    "    return Markdown(md_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84146a62-33e6-44ff-90a3-568e995eda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sigma_t'] = data.pop('input1')\n",
    "data['delta_epsilon'] = data.pop('input2')\n",
    "data['delta_sigma'] = data.pop('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9024e838-c04e-4ced-bbc9-fba2fe1d9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_delta_sigma(delta_sigma_pred, scaler):\n",
    "    \"\"\"\n",
    "    Denormalisiert die Vorhersagewerte für delta_sigma zurück auf den ursprünglichen Bereich.\n",
    "    \n",
    "    :param delta_sigma_pred: Normalisierte Vorhersagewerte für `delta_sigma`\n",
    "    :param scaler: Der MinMaxScaler, der für die Normalisierung verwendet wurde\n",
    "    :return: Denormalisierte Werte für `delta_sigma_pred`\n",
    "    \"\"\"\n",
    "    # Denormalisieren basierend auf den min und max Werten des Scalers\n",
    "    delta_sigma_pred_denorm = delta_sigma_pred * (scaler.data_max_[2] - scaler.data_min_[2]) + scaler.data_min_[2]\n",
    "    return delta_sigma_pred_denorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9396410f-9a29-4425-8d7c-f6c13642323a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Data-Loss bis sigma_19\n",
       "\n",
       "| Index | delta_epsilon | sigma_t | True delta_sigma | Predicted delta_sigma | Test-Loss (True - Predicted) |\n",
       "|--|--------------|--------|-----------------|----------------------|-----------------------------|\n",
       "| 0 | 0.0005 | 1.0 | 0.2 | 4088.7202 | -4088.5202 |\n",
       "| 1 | 0.0005 | 1.2 | 0.24 | 4219.3274 | -4219.0874 |\n",
       "| 2 | 0.0005 | 1.44 | 0.288 | 4376.3646 | -4376.0766 |\n",
       "| 3 | 0.0005 | 1.728 | 0.3456 | 4564.151 | -4563.8054 |\n",
       "| 4 | 0.0005 | 2.0736 | 0.4147 | 4789.9884 | -4789.5737 |\n",
       "| 5 | 0.0005 | 2.4883 | 0.4977 | 5060.9727 | -5060.475 |\n",
       "| 6 | 0.0005 | 2.986 | 0.5972 | 5386.051 | -5385.4538 |\n",
       "| 7 | 0.0005 | 3.5832 | 0.7166 | 5776.2272 | -5775.5106 |\n",
       "| 8 | 0.0005 | 4.2998 | 0.86 | 6244.3564 | -6243.4964 |\n",
       "| 9 | 0.0005 | 5.1598 | 1.032 | 6806.1731 | -6805.1411 |\n",
       "| 10 | 0.0005 | 6.1917 | 1.2383 | 7480.1886 | -7478.9503 |\n",
       "| 11 | 0.0005 | 7.4301 | 1.486 | 8289.2335 | -8287.7475 |\n",
       "| 12 | 0.0005 | 8.9161 | 1.7832 | 9260.0462 | -9258.263 |\n",
       "| 13 | 0.0005 | 10.6993 | 2.1399 | 10424.9187 | -10422.7788 |\n",
       "| 14 | 0.0005 | 12.8392 | 2.5678 | 11822.8273 | -11820.2594 |\n",
       "| 15 | 0.0005 | 15.407 | 3.0814 | 13500.3588 | -13497.2774 |\n",
       "| 16 | 0.0005 | 18.4884 | 3.6977 | 15513.3554 | -15509.6577 |\n",
       "| 17 | 0.0005 | 22.1861 | 4.4372 | 17823.1492 | -17818.712 |\n",
       "| 18 | 0.0005 | 26.6233 | 5.3247 | 20552.5316 | -20547.2069 |\n",
       "| 19 | 0.0005 | 31.948 | 6.3896 | 24170.2488 | -24163.8592 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "![Prediction vs True Solution](./graph/visual_prediction-vs-truesolution_comp0.png)<br>**Hinweis:** Datenpunkte liegen sehr nahe beieinander."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_folder = 'graph'\n",
    "img_extensions = '.png'\n",
    "\n",
    "img_visual_prediction_vs_truesolution_comp0 = 'visual_prediction-vs-truesolution_comp0'\n",
    "# print(data)\n",
    "\n",
    "# Erstelle die Eingabedaten als LabelTensor für das trainierte Modell\n",
    "input_data = LabelTensor(X_test, ['sigma_t', 'delta_epsilon'])\n",
    "\n",
    "delta_sigma_pred = denormalize_delta_sigma(pinn(input_data).detach().numpy(), scaler)\n",
    "\n",
    "display_data_loss_table(data_dict=data, delta_sigma_pred=delta_sigma_pred, max_i=20)\n",
    "plot_prediction_vs_true_solution(pinn=pinn, data_dict=data, graph_folder=graph_folder, img_visual_prediction_vs_truesolution=img_visual_prediction_vs_truesolution_comp0, \n",
    "                                     img_extensions=img_extensions, y_axis='delta_epsilon', max_i=20, plot_type=\"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586404fb-750c-4b98-8f33-d9681f48bcab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
