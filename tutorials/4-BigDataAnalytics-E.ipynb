{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "- After finishing the second exercise you should be familiar with data loading, data inspection and simple models including their evaluation.\n",
    "- Building up on this technical understanding, we will combine it with the theoretical progress in lecture two to four. Next to other things you should have learned about the basics of simple neural networks and their evaluation. This will be the focus point of the following exercise. \n",
    "- In the source code, gaps, which are marked with **N TODO**, have to be filled and theoretical questions have to be answered. For experimentation, further changes to the code should be made at the end, if requested. In total we got **17 TODO**s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules & Check PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# torch modules\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right at the beginning: check if a cuda compatible GPU is available in your computer. If so, set device = cuda:0, which means that later all calculations will be performed on the graphics card.  If no GPU is available, the calculations will run on the CPU, which is also absolutely sufficient for the examples in these exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    device_num = 0\n",
    "    print('No GPU available.')\n",
    "else:\n",
    "    device_num = torch.cuda.device_count()\n",
    "    print('Device:', device, '-- Number of devices:', device_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression network\n",
    "The first neural network you develop will be a regression network. For this task, the following source code is incomplete, with which life expectancy can be estimated on the basis of certain features collected from the WHO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data set ”LifeExpectancyData.csv” and (**1. TODO**) print its size, which is the number of samples and its dimension, which is the number of features per sample. It may be necessary to adjust the path for reading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the dataset: ['Country' 'Year' 'Status' 'Life expectancy ' 'Adult Mortality'\n",
      " 'infant deaths' 'Alcohol' 'percentage expenditure' 'Hepatitis B'\n",
      " 'Measles ' ' BMI ' 'under-five deaths ' 'Polio' 'Total expenditure'\n",
      " 'Diphtheria ' ' HIV/AIDS' 'GDP' 'Population' ' thinness  1-19 years'\n",
      " ' thinness 5-9 years' 'Income composition of resources' 'Schooling']\n"
     ]
    }
   ],
   "source": [
    "# Path to WHO data\n",
    "data_path = '../data/LifeExpectancyData.csv'\n",
    "\n",
    "# Read csv sheet with pandas\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Get numpy out of pandas dataframe\n",
    "data = df.values\n",
    "\n",
    "# Get column names to see, which columns we have to extract as x and y\n",
    "column_names = np.array(df.columns)\n",
    "\n",
    "# TODO\n",
    "print('Dimension of the dataset:', column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data set into input $x$ and target $y$ features (**2. TODO**). For a start, the BMI column shall be used as $x$ and the column with the life expectancy as $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (1649,)\n",
      "y shape: (1649,)\n"
     ]
    }
   ],
   "source": [
    "# Split in X and Y\n",
    "\n",
    "# TODO**\n",
    "x = np.array(data[:, 10], dtype=np.float32)\n",
    "y = np.array(data[:, 3], dtype=np.float32)\n",
    "\n",
    "print('x shape:', x.shape)\n",
    "print('y shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following $x$ and $y$ are normalized, which is very important for the stability of neural networks. If several input features were used, those in lower value ranges would possibly be suppressed without normalization.\n",
    "They are also converted to torch tensors $x \\rightarrow X$ and $y \\rightarrow Y$, so that they can later pass through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale_x: 77.1\n",
      "Scale_y: 89.0\n"
     ]
    }
   ],
   "source": [
    "# If multiple features in X are selected, each feature is normalized individually\n",
    "scale_x = np.max(x, axis=0)\n",
    "scale_y = np.max(y, axis=0)\n",
    "x = x/scale_x\n",
    "y = y/scale_y\n",
    "print('Scale_x:',scale_x)\n",
    "print('Scale_y:',scale_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors\n",
    "# If tensors have only one dimension, an artificial dimension is created with unsqueeze (e.g. [10]->[10,1], so 1D->2D)\n",
    "Y = torch.from_numpy(y)\n",
    "Y = Y.float()\n",
    "if len(Y.shape)==1:\n",
    "    Y = Y.unsqueeze(dim=1)\n",
    "\n",
    "X = torch.from_numpy(x)\n",
    "X = X.float()\n",
    "if len(X.shape)==1:\n",
    "    X = X.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide $X$ and $Y$ into a training, validation and test set . Determine the division of the sets appropriately.\n",
    "The proportions must add up to 1. Explain, why a split in training, validation and test set is important (**3. TODO**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of first ten train Sample: tensor([[0.6031],\n",
      "        [0.2918],\n",
      "        [0.4721],\n",
      "        [0.5694],\n",
      "        [0.6641],\n",
      "        [0.2309],\n",
      "        [0.7588],\n",
      "        [0.6394],\n",
      "        [0.0661],\n",
      "        [0.7224]])\n",
      "Target of first ten train Sample: tensor([[0.8202],\n",
      "        [0.7360],\n",
      "        [0.7348],\n",
      "        [0.7404],\n",
      "        [0.8449],\n",
      "        [0.7506],\n",
      "        [0.8180],\n",
      "        [0.8730],\n",
      "        [0.8225],\n",
      "        [0.8966]])\n"
     ]
    }
   ],
   "source": [
    "# Split dataset in training, validation and test tensors\n",
    "# TODO**\n",
    "prop_train = 0.8\n",
    "prop_val = 0.1\n",
    "prop_test = 0.1\n",
    "\n",
    "sample_num = {'all': X.shape[0], \n",
    "              'train': round(prop_train*X.shape[0]),\n",
    "              'val': round(prop_val*X.shape[0]),\n",
    "              'test': round(prop_test*X.shape[0])}\n",
    "\n",
    "# idx shuffle\n",
    "idx = np.random.choice(sample_num['all'], sample_num['all'], replace=False)\n",
    "# assign idx to each sample\n",
    "sample_idx = {'all': idx[:], \n",
    "              'train': idx[0:sample_num['train']],\n",
    "              'val': idx[sample_num['train']:sample_num['train']+sample_num['val']],\n",
    "              'test': idx[sample_num['train']+sample_num['val']:]}\n",
    "\n",
    "# Create train data\n",
    "X_train = X[sample_idx['train']]\n",
    "Y_train = Y[sample_idx['train']]\n",
    "\n",
    "# Create validation data\n",
    "X_val = X[sample_idx['val']]\n",
    "Y_val = Y[sample_idx['val']]\n",
    "\n",
    "# Create test data\n",
    "X_test = X[sample_idx['test']]\n",
    "Y_test = Y[sample_idx['test']]\n",
    "\n",
    "\n",
    "# Show data point\n",
    "print('Input of first ten train Sample:', X_train[0:10])\n",
    "print('Target of first ten train Sample:', Y_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build neural network\n",
    "\n",
    "After preparing the data we can now start designing the architecture of our regression network.\n",
    "\n",
    "Look at the RegressNet class. It consists of 2 methods. *init(self, inputSize, outputSize)* specifies the layers that this class holds. *forward(self, x)* specifies the connection between these layers. Note that activation functions are also used here. Describe the architecture of the network with the technical vocabulary you learned in the lecture (**4. TODO**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class of neural network 'RegressNet'\n",
    "# Set up layer and architecture of network in constructor __init__\n",
    "# Define operations on layer in forward pass method\n",
    "\n",
    "class RegressNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(RegressNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, outputSize)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # max pooling over (2, 2) window\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider which input and output dimensions the network needs to have in the current case. Afterwards create an instance net of RegressNet (**5. TODO**).\n",
    "\n",
    "Hint: Think about the dimensions of $X$ and $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressNet(\n",
      "  (fc1): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Specify network hyperparameter and create instance of RegressNet\n",
    "# TODO**        \n",
    "inputDim = 1\n",
    "outputDim = 1\n",
    "\n",
    "# Create instance of RegressNet\n",
    "net = RegressNet(inputDim, outputDim)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a graphics card is available, $X, Y$ and net are now sent to the GPU for efficient computation. Otherwise the network is trained on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressNet(\n",
       "  (fc1): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send tensors and networks to GPU (if you have one which supports cuda) for faster computations\n",
    "X_train, Y_train = X_train.to(device), Y_train.to(device)\n",
    "X_val, Y_val = X_val.to(device), Y_val.to(device)\n",
    "X_test, Y_test = X_test.to(device), Y_test.to(device)\n",
    "\n",
    "# The network itself must also be sent to the GPU. Either you write net = RegressNet() and then later net.to(device) or directly net = RegressNet().to(device)\n",
    "# The latter option may have the advantage that the instance net is created directly on the GPU, whereas in variant 1 it must first be sent to the GPU.\n",
    "if device_num>1:\n",
    "    print(\"Let's use\", device_num, \"GPU's\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network\n",
    "\n",
    "In order to train the network, the so-called hyperparameters must be defined. In this case it is the number of epochs (*num_epoch*) the network should be trained, the learning rate (*learn_rate*), the loss function and the optimizer. The latter two have already been defined. We use a mean squared error loss function MSELoss() and ADAM optimizer (**6. TODO**).\n",
    "\n",
    "Hint: The learning rate is usually in a range from 1e-1 to 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyperparameter\n",
    "# Hyperparemter: num_epoch, num_lr, loss_func, optimizer\n",
    "\n",
    "# TODO** \n",
    "num_epoch = 100\n",
    "learn_rate = 1e-5\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss before training\n",
    "# Compute loss of test data before training the network (with random weights)\n",
    "Y_pred_train_before = net(X_train)\n",
    "loss_train_before = loss_func(Y_pred_train_before, Y_train)\n",
    "Y_pred_val_before = net(X_val)\n",
    "loss_val_before = loss_func(Y_pred_val_before, Y_val)\n",
    "Y_pred_test_before = net(X_test)\n",
    "loss_test_before = loss_func(Y_pred_test_before, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now follows the training. In a loop over the epochs, the training data passes through the network, which is called forward pass. The training loss is calculated from the forward pass prediction and the reference. Then the network\n",
    "is optimized by backpropagation.\n",
    "\n",
    "For comparison purposes, the forward pass and the loss are also calculated for the validation set within the same loop in each epoch. Insert the calculation for *y_pred_val* and *loss_val* (**7. TODO**).\n",
    "\n",
    "In order to evaluate the training process we monitor how the loss changes with the number of epochs. Thereby both, train and validation loss is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# Monitor loss curve during training\n",
    "plt.figure() \n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    # Classical forward pass -> predict new output from train data\n",
    "    Y_pred_train = net(X_train)\n",
    "    # Compute loss    \n",
    "    loss_train = loss_func(Y_pred_train, Y_train)\n",
    "    \n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter. This is why you should call optimizer.zero_grad() after each .step() call\n",
    "    # Note that following the first .backward call, a second call is only possible after you have performed another forward pass.\n",
    "    loss_train.backward()\n",
    "    # Perform a parameter update based on the current gradient (stored in .grad attribute of a parameter)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # TODO**\n",
    "    # forward pass for validation\n",
    "    Y_pred_val = net(X_val)\n",
    "    loss_val = loss_func(Y_pred_val, Y_val)\n",
    "    \n",
    "    # Plot train and val loss\n",
    "    plt.scatter(epoch, loss_train.data.item(), color='b', s=10, marker='o')    \n",
    "    plt.scatter(epoch, loss_val.data.item(), color='r', s=10, marker='o')\n",
    "    \n",
    "    # Print message with actual losses\n",
    "    print('Train Epoch: {}/{} ({:.0f}%)\\ttrain_Loss: {:.6f}\\tval_Loss: {:.6f}'.format(\n",
    "    epoch+1, num_epoch, epoch/num_epoch*100, loss_train.item(), loss_val.item()))\n",
    "\n",
    "\n",
    "# Show training and validation loss    \n",
    "plt.legend(['train-loss','val-loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('../results/who_loss.png')\n",
    "#plt.show()\n",
    "\n",
    "print('Train loss before training was:', loss_train_before.item())\n",
    "print('Train loss after training is:', loss_train.item())\n",
    "print('Val loss before training was:', loss_val_before.item())\n",
    "print('Val loss after training is:', loss_val.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another figure that compares the predicted life expectancy with the reference. With a perfect model, these values are identical, i.e. all points are on the black line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pred vs. Ref Figure Train/Val set\n",
    "# Plot the prediction against the reference for the train/val points\n",
    "\n",
    "plt.figure()\n",
    "plt.title('pred vs. ref: train/val points')\n",
    "plt.scatter(Y_train.cpu().numpy(), Y_pred_train.cpu().detach().numpy(), color='b', s=5, marker='o')\n",
    "plt.scatter(Y_val.cpu().numpy(), Y_pred_val.cpu().detach().numpy(), color='r', s=5, marker='o')\n",
    "plt.scatter(Y_val.cpu().numpy(), Y_pred_val_before.cpu().detach().numpy(), color='m', s=5, marker='^')\n",
    "plt.plot((0,1),(0,1), color='k')\n",
    "plt.xlabel('reference')\n",
    "plt.ylabel('prediction')\n",
    "plt.legend(['perfect model', 'train-sample after tr','val-sample after tr', 'val-sample before tr'])\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.savefig('../results/who_pred_vs_ref_val.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. TODO**: Try to analyze the loss curves: How can you see from the loss curves how many epochs you should train a network? Please discuss the training loss as well as the validation loss. Therefore vary the hyperparameters learn rate and num epoch and look at the different loss curves.\n",
    "\n",
    "\n",
    "**9. TODO**: To further optimize the network, the architecture of the network can be improved: Change the net depth (vary number of hidden layer) and width (vary number of neurons in hidden layer) in the class RegressNet. Then briefly describe the influence on the loss curves and computing time with unchanged hyperparameters.\n",
    "\n",
    "Hint 1: PyTorch documentation (https://pytorch.org/docs/stable/nn.html) is useful for more information\n",
    "about the layers.\n",
    "\n",
    "Hint 2: Afterwards the hyperparameters should be adjusted to fit the new architecture.\n",
    "\n",
    "**10. TODO**: Improve the model by predicting life expectancy not only as a function of BMI, but of multiple features. Use any number of other features combined as $X$. To do this, you have to go back to the point where you split the data into $$X$ and $Y$. Do not use the first three columns, since they contain no float values or only metadata. \n",
    "\n",
    "Important: The input dimension of the network must also be adjusted. \n",
    "\n",
    "Note: If you make a change to an already executed cell in the .ipynb script, all other cells should also be executed again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Finally, the trained neural network can be tested for new/unseen data.\n",
    "\n",
    "**11. TODO**: Insert the calculation of *y_pred_test* and *loss_test*. Look also at the mean absolute difference between the predicted and the reference life expectancy from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results\n",
    "# TODO**\n",
    "Y_pred_test = net(X_test)\n",
    "loss_test = loss_func(Y_pred_test, Y_test)\n",
    "\n",
    "print('Test loss before training was:', loss_test_before.item())\n",
    "print('Test loss after training is:', loss_test.item())\n",
    "\n",
    "# Plot mean abs difference between prediction and reference\n",
    "print('Mean abs difference:', np.mean(abs(Y_pred_test.cpu().detach().numpy()-Y_test.cpu().numpy()), axis=0)*scale_y, 'years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Network\n",
    "\n",
    "The step to the first classification network is not big anymore. For this task, you are provided with the source code, which is very similar to that of the regression network. The iris-flower dataset consists of 160 iris, each described by 4 features and divided into 3 different classes. The aim is to use a simple neural classification network to predict the class from the features.\n",
    "\n",
    "- Solve the **TODO**s analogous to the regression network. Note that we now have to work with a one-hot encoded ([take a look here](https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179))\n",
    "target vector.\n",
    "- The evaluation is slightly modified: Along with the loss plots we use a confusion matrix now to compare reference and prediction.\n",
    "\n",
    "\n",
    "**18. TODO**: It may be that the classification will not yet work optimally. This is because there is no activation function above the output of the network. Explain, why it makes sense to use a softmax activation for a classification task. Build such an activation with *F.softmax(output, dim=1)* into the architecture and compare the *y_pred* after the respective runs with and without activation. Note: Instead of manually adding a softmax function, you can also use another loss function e.g. CrossEntropyLoss(), which applies the softmax activation internally. We will use this in the following exercise. It might make sense to solve this **TODO** after all the other **TODO**...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data_path = '../data/iris.data'\n",
    "\n",
    "# Read csv sheet with pandas\n",
    "df = pd.read_csv(data_path, sep=',')\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Get numpy out of pandas dataframe\n",
    "data = df.values\n",
    "\n",
    "column_names = np.array(df.columns[:])\n",
    "\n",
    "# TODO**\n",
    "print('Dimensions of the dataset:',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in X and Y\n",
    "x = np.array(data[:,:-1], dtype=np.float32)\n",
    "\n",
    "y = pd.factorize(data[:,-1])[0]\n",
    "\n",
    "class_names = np.unique(data[:,-1])\n",
    "\n",
    "# Save number of classes\n",
    "nc = np.max(y)+1\n",
    "\n",
    "print('x shape:', x.shape)\n",
    "print('y shape:', y.shape)\n",
    "print('number of classes', nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize X between (0,1). If multiple features in X are selected, each feature is normalized individually\n",
    "scale_x = np.max(x, axis=0)\n",
    "x = x/scale_x\n",
    "print('Scale_x:',scale_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors\n",
    "# if tensors have only one dimension, an artificial dimension is created with unsqueeze (e.g. [10]->[10,1], so 1D->2D)\n",
    "Y = torch.from_numpy(y)\n",
    "Y = Y.long()\n",
    "\n",
    "# Produce onehot target tensor\n",
    "# scatter_ mehtod fills the tensor with values from a source tensor along the indices provided as arguments\n",
    "# oh = one hot encoding\n",
    "Y_oh = torch.zeros(Y.shape[0], nc)\n",
    "Y_oh.scatter_(1,Y.unsqueeze(1), 1.0)\n",
    "\n",
    "X = torch.from_numpy(x)\n",
    "X = X.float()\n",
    "if len(X.shape)==1:\n",
    "    X = X.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. TODO**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in training, validation and test tensors\n",
    "# TODO**\n",
    "prop_train = 0.8\n",
    "prop_val = 0.1\n",
    "prop_test = 0.1\n",
    "\n",
    "sample_num = {'all': X.shape[0], \n",
    "              'train': round(prop_train*X.shape[0]),\n",
    "              'val': round(prop_val*X.shape[0]),\n",
    "              'test': round(prop_test*X.shape[0])}\n",
    "\n",
    "# idx shuffle\n",
    "idx = np.random.choice(sample_num['all'], sample_num['all'], replace=False)\n",
    "\n",
    "# Assign idx to each sample\n",
    "sample_idx = {'all': idx[:], \n",
    "              'train': idx[0:sample_num['train']],\n",
    "              'val': idx[sample_num['train']:sample_num['train']+sample_num['val']],\n",
    "              'test': idx[sample_num['train']+sample_num['val']:]}\n",
    "\n",
    "# Create train data\n",
    "X_train = X[sample_idx['train']]\n",
    "Y_train_oh = Y_oh[sample_idx['train']]\n",
    "Y_train = Y[sample_idx['train']]\n",
    "\n",
    "# Create validation data\n",
    "X_val = X[sample_idx['val']]\n",
    "Y_val_oh = Y_oh[sample_idx['val']]\n",
    "Y_val = Y[sample_idx['val']]\n",
    "\n",
    "# Create test data\n",
    "X_test = X[sample_idx['test']]\n",
    "Y_test_oh = Y_oh[sample_idx['test']]\n",
    "Y_test = Y[sample_idx['test']]\n",
    "\n",
    "# Show data point\n",
    "print('Input of first ten train Sample:', X_train[0:10])\n",
    "print('Target of first ten train Sample:', Y_train[0:10])\n",
    "print('One-Hot-Encoded Target of first ten train Sample:', Y_train_oh[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build classification neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class of neural network 'ClassificationNet'\n",
    "# Set up layer and architecture of network in constructor __init__\n",
    "# Define operations on layer in forward pass method\n",
    "\n",
    "class ClassificationNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, outputSize)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # max pooling over (2, 2) window\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. TODO**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify network parameter\n",
    "# TODO**  \n",
    "inputDim = 4\n",
    "outputDim = 3\n",
    " \n",
    "# Create instance of ClassificationNet\n",
    "net = ClassificationNet(inputDim, outputDim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send tensors and networks to GPU (if you have one which supports cuda) for faster computations\n",
    "# Note: Y is one-hot-encoded\n",
    "X_train, Y_train_oh = X_train.to(device), Y_train_oh.to(device)\n",
    "X_val, Y_val_oh = X_val.to(device), Y_val_oh.to(device)\n",
    "X_test, Y_test_oh = X_test.to(device), Y_test_oh.to(device)\n",
    "\n",
    "# The network itself must also be sent to the GPU. Either you write net = RegressNet() and then later net.to(device) or directly net = RegressNet().to(device)\n",
    "# The latter option may have the advantage that the instance net is created directly on the GPU, whereas in variant 1 it must first be sent to the GPU.\n",
    "if device_num>1:\n",
    "    print(\"Let's use\", device_num, \"GPU's\")\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device) \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15. TODO**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyperparameter\n",
    "# Hyperparemter: num_epoch, num_lr, loss_func, optimizer\n",
    "# TODO**  \n",
    "num_epoch = 2000\n",
    "num_lr = 1e-4\n",
    "\n",
    "# Loss and optimizer\n",
    "# loss_func = nn.MSELoss() # -> one hot encoded 'target' to loss-function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=num_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Accuracy before training\n",
    "# Compute loss of test data before training the network (with random weights)\n",
    "\n",
    "Y_pred_test_before_oh = net(X_test)\n",
    "# Loss function input looks as follows: loss_func(prediction, target)\n",
    "# Note: for CrossEntropyLoss(): prediction is one_hot_encoded, target has single dimension\n",
    "# for MSELoss(): target and loss has to be both one_hot_encoded \n",
    "\n",
    "loss_test_before = loss_func(Y_pred_test_before_oh, Y_test_oh)\n",
    "\n",
    "# Accuracy before training\n",
    "y_pred_test_before = np.argmax(Y_pred_test_before_oh.cpu().detach().numpy(), axis=1)\n",
    "correct_before = np.sum(y_pred_test_before == Y_test.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16. TODO**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "plt.figure() \n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    # classical forward pass -> predict new output from train data\n",
    "    Y_pred_train_oh = net(X_train)\n",
    "    # Compute loss    \n",
    "    loss_train = loss_func(Y_pred_train_oh, Y_train_oh)\n",
    "    \n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Note: Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter. This is why you should call optimizer.zero_grad() after each .step() call\n",
    "    # Note that following the first .backward call, a second call is only possible after you have performed another forward pass.\n",
    "    loss_train.backward()\n",
    "    # Perform a parameter update based on the current gradient (stored in .grad attribute of a parameter)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # TODO**\n",
    "    # Forward pass for validation\n",
    "    Y_pred_val_oh = net(X_val)\n",
    "    loss_val = loss_func(Y_pred_val_oh, Y_val_oh)\n",
    "    \n",
    "    # Compute actual train accuracy\n",
    "    y_pred_train = np.argmax(Y_pred_train_oh.cpu().detach().numpy(), axis=1)\n",
    "    correct_train = np.sum(y_pred_train == Y_train.numpy())\n",
    "    \n",
    "    # Compute actual val accuracy\n",
    "    y_pred_val = np.argmax(Y_pred_val_oh.cpu().detach().numpy(), axis=1)\n",
    "    correct_val = np.sum(y_pred_val == Y_val.numpy())\n",
    "    \n",
    "    # Plot train and val loss and accuracies\n",
    "    plt.scatter(epoch, loss_train.data.item(), color='r', s=10, marker='o')\n",
    "    plt.scatter(epoch, loss_val.data.item(), color='b', s=10, marker='o')\n",
    "    plt.scatter(epoch, correct_train/Y_train.shape[0], color='m', s=10, marker='o') \n",
    "    plt.scatter(epoch, correct_val/Y_val.shape[0], color='c', s=10, marker='o')\n",
    "    \n",
    "    # Print message with actual losses\n",
    "    print('Train Epoch: {}/{} ({:.0f}%)\\ttrain_Loss: {:.6f}\\tval_Loss: {:.6f}'.format(\n",
    "    epoch+1, num_epoch, epoch/num_epoch*100, loss_train.item(), loss_val.item()))\n",
    "       \n",
    "\n",
    "# Show training and validation loss    \n",
    "plt.legend(['train-loss','val-loss','train-acc','val-acc'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('../results/irisflower_loss.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17. TODO**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results\n",
    "# TODO**\n",
    "# Forward pass \n",
    "# Y_pred_test_oh is on the GPU, because net and X_test are on the GPU, but we want it on the CPU from now on.\n",
    "Y_pred_test_oh = net(X_test)\n",
    "# Compute and print losses\n",
    "loss_test = loss_func(Y_pred_test_oh, Y_test_oh)\n",
    "\n",
    "print('Test loss before training was:', loss_test_before.item())\n",
    "print('Test loss after training is:', loss_test.item())\n",
    "\n",
    "# Compute and print accuracies\n",
    "y_pred_test = np.argmax(Y_pred_test_oh.cpu().detach().numpy(), axis=1)\n",
    "correct = np.sum(y_pred_test == Y_test.numpy())\n",
    "print('Test accuracy before training: ', correct_before/Y_test.shape[0]*100, '%')\n",
    "print('Test accuracy after training: ', correct/Y_test.shape[0]*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation module\n",
    "clf = SVC(random_state=0)\n",
    "clf.fit(X_train, y_pred_train)\n",
    "SVC(random_state=0)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Plot test confusion matrix\n",
    "cm = confusion_matrix(y_pred_test, predictions, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "plt.savefig('../results/irisflower_confusion.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
